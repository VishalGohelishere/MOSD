{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "current_key = None\n",
    "current_values = []\n",
    "\n",
    "with open('Earthquakes_TRAIN.ts', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith('@'):\n",
    "            if current_key is not None:\n",
    "                if len(current_values) == 1:\n",
    "                    data_dict[current_key] = current_values[0]\n",
    "                else:\n",
    "                    data_dict[current_key] = current_values\n",
    "                current_values = []\n",
    "            current_key = line\n",
    "        else:\n",
    "            current_values.append(line)\n",
    "\n",
    "if current_key is not None:\n",
    "    if len(current_values) == 1:\n",
    "        data_dict[current_key] = current_values[0]\n",
    "    else:\n",
    "        data_dict[current_key] = current_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]\n",
    "\n",
    "pdfeatures0 = [list(map(float, item[0].split(','))) for item in split_data]\n",
    "# pdfeatures1 = [list(map(float, item[1].split(','))) for item in split_data]\n",
    "# pdfeatures2 = [list(map(float, item[2].split(','))) for item in split_data]\n",
    "# pdfeatures3 = [list(map(float, item[3].split(','))) for item in split_data]\n",
    "# pdfeatures4 = [list(map(float, item[4].split(','))) for item in split_data]\n",
    "# pdfeatures5 = [list(map(float, item[5].split(','))) for item in split_data]\n",
    "# pdfeatures6 = [list(map(float, item[6].split(','))) for item in split_data]\n",
    "# pdfeatures7 = [list(map(float, item[7].split(','))) for item in split_data]\n",
    "# pdfeatures8 = [list(map(float, item[8].split(','))) for item in split_data]\n",
    "# pdfeatures9 = [list(map(float, item[9].split(','))) for item in split_data]\n",
    "# pdfeatures10 = [list(map(float, item[10].split(','))) for item in split_data]\n",
    "# pdfeatures11 = [list(map(float, item[11].split(','))) for item in split_data]\n",
    "# pdfeatures12 = [list(map(float, item[12].split(','))) for item in split_data]\n",
    "# pdfeatures13 = [list(map(float, item[13].split(','))) for item in split_data]\n",
    "\n",
    "features0 = np.array([list(map(float, item[0].split(','))) for item in split_data])\n",
    "# features1 = np.array([list(map(float, item[1].split(','))) for item in split_data])\n",
    "# features2 = np.array([list(map(float, item[2].split(','))) for item in split_data])\n",
    "# features3 = np.array([list(map(float, item[3].split(','))) for item in split_data])\n",
    "# features4 = np.array([list(map(float, item[4].split(','))) for item in split_data])\n",
    "# features5 = np.array([list(map(float, item[5].split(','))) for item in split_data])\n",
    "# features6 = np.array([list(map(float, item[6].split(','))) for item in split_data])\n",
    "# features7 = np.array([list(map(float, item[7].split(','))) for item in split_data])\n",
    "# features8 = np.array([list(map(float, item[8].split(','))) for item in split_data])\n",
    "# features9 = np.array([list(map(float, item[9].split(','))) for item in split_data])\n",
    "# features10 = np.array([list(map(float, item[10].split(','))) for item in split_data])\n",
    "# features11 = np.array([list(map(float, item[11].split(','))) for item in split_data])\n",
    "# features12 = np.array([list(map(float, item[12].split(','))) for item in split_data])\n",
    "# features13 = np.array([list(map(float, item[13].split(','))) for item in split_data])\n",
    "\n",
    "labels = [0 if int(item[-1]) else 1 for item in split_data]\n",
    "\n",
    "pddf = pd.DataFrame({'F1': pdfeatures0,\n",
    "                  #  'F2': pdfeatures1,\n",
    "                  #  'F3': pdfeatures2,\n",
    "                  #  'F4': pdfeatures3,\n",
    "                #    'F5': pdfeatures4,\n",
    "                #    'F6': pdfeatures5,\n",
    "                #    'F7': pdfeatures6,\n",
    "                #    'F8': pdfeatures7,\n",
    "                #    'F9': pdfeatures8,\n",
    "                #    'F10': pdfeatures9,\n",
    "                #    'F11': pdfeatures10,\n",
    "                #    'F12': pdfeatures11,\n",
    "                #    'F13': pdfeatures12,\n",
    "                #    'F14': pdfeatures13,\n",
    "                   'Labels': labels})\n",
    "df = np.array(features0)#, features1, features2, features3], dtype=float)#, features4, features5, features6, features7, features8, features9, features10, features11, features12, features13], dtype=float)\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    135\n",
       "0     26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(labels[161:]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = torch.tensor(df).float().unsqueeze(1)\n",
    "df_labels = torch.tensor(labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features, df_train_labels, df_test_features, df_test_labels = df_features[:161,:], df_labels[:161], df_features[161:,:], df_labels[161:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "        # batch_first=True (batch_dim, seq_dim, feature_dim)\n",
    "        self.drp = nn.Dropout(0.7)\n",
    "        self.fc = nn.Linear(hidden_d, hidden_d)\n",
    "        self.relu = nn.ReLU(hidden_d)\n",
    "        self.fc2 = nn.Linear(hidden_d, output_d)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.drp(out)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return self.sig(self.fc2(self.relu(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6931472420692444\n",
      "Validation Accuracy: 0.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vishal Gohel\\Vishal Gohel\\Models of Sequential Data\\project\\classification.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(df_train_features)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mfloat(), df_train_labels\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Vishal Gohel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Vishal Gohel\\Vishal Gohel\\Models of Sequential Data\\project\\classification.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_dim, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_dim, x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_dim)\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     out, (hn, cn) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x, (h0\u001b[39m.\u001b[39mdetach(), c0\u001b[39m.\u001b[39mdetach()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrp(out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_d=512, hidden_d=100, layer_d=100, output_d=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(df_train_features)\n",
    "    loss = criterion(outputs.float(), df_train_labels.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Training loss: {loss}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        outputs = model( df_test_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(df_test_labels, predicted)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
