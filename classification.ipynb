{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "current_key = None\n",
    "current_values = []\n",
    "\n",
    "with open('Earthquakes_TRAIN.ts', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith('@'):\n",
    "            if current_key is not None:\n",
    "                if len(current_values) == 1:\n",
    "                    data_dict[current_key] = current_values[0]\n",
    "                else:\n",
    "                    data_dict[current_key] = current_values\n",
    "                current_values = []\n",
    "            current_key = line\n",
    "        else:\n",
    "            current_values.append(line)\n",
    "\n",
    "if current_key is not None:\n",
    "    if len(current_values) == 1:\n",
    "        data_dict[current_key] = current_values[0]\n",
    "    else:\n",
    "        data_dict[current_key] = current_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]\n",
    "\n",
    "pdfeatures0 = [list(map(float, item[0].split(','))) for item in split_data]\n",
    "# pdfeatures1 = [list(map(float, item[1].split(','))) for item in split_data]\n",
    "# pdfeatures2 = [list(map(float, item[2].split(','))) for item in split_data]\n",
    "# pdfeatures3 = [list(map(float, item[3].split(','))) for item in split_data]\n",
    "# pdfeatures4 = [list(map(float, item[4].split(','))) for item in split_data]\n",
    "# pdfeatures5 = [list(map(float, item[5].split(','))) for item in split_data]\n",
    "# pdfeatures6 = [list(map(float, item[6].split(','))) for item in split_data]\n",
    "# pdfeatures7 = [list(map(float, item[7].split(','))) for item in split_data]\n",
    "# pdfeatures8 = [list(map(float, item[8].split(','))) for item in split_data]\n",
    "# pdfeatures9 = [list(map(float, item[9].split(','))) for item in split_data]\n",
    "# pdfeatures10 = [list(map(float, item[10].split(','))) for item in split_data]\n",
    "# pdfeatures11 = [list(map(float, item[11].split(','))) for item in split_data]\n",
    "# pdfeatures12 = [list(map(float, item[12].split(','))) for item in split_data]\n",
    "# pdfeatures13 = [list(map(float, item[13].split(','))) for item in split_data]\n",
    "\n",
    "features0 = np.array([list(map(float, item[0].split(','))) for item in split_data])\n",
    "# features1 = np.array([list(map(float, item[1].split(','))) for item in split_data])\n",
    "# features2 = np.array([list(map(float, item[2].split(','))) for item in split_data])\n",
    "# features3 = np.array([list(map(float, item[3].split(','))) for item in split_data])\n",
    "# features4 = np.array([list(map(float, item[4].split(','))) for item in split_data])\n",
    "# features5 = np.array([list(map(float, item[5].split(','))) for item in split_data])\n",
    "# features6 = np.array([list(map(float, item[6].split(','))) for item in split_data])\n",
    "# features7 = np.array([list(map(float, item[7].split(','))) for item in split_data])\n",
    "# features8 = np.array([list(map(float, item[8].split(','))) for item in split_data])\n",
    "# features9 = np.array([list(map(float, item[9].split(','))) for item in split_data])\n",
    "# features10 = np.array([list(map(float, item[10].split(','))) for item in split_data])\n",
    "# features11 = np.array([list(map(float, item[11].split(','))) for item in split_data])\n",
    "# features12 = np.array([list(map(float, item[12].split(','))) for item in split_data])\n",
    "# features13 = np.array([list(map(float, item[13].split(','))) for item in split_data])\n",
    "\n",
    "labels = [0 if int(item[-1]) else 1 for item in split_data]\n",
    "\n",
    "pddf = pd.DataFrame({'F1': pdfeatures0,\n",
    "                  #  'F2': pdfeatures1,\n",
    "                  #  'F3': pdfeatures2,\n",
    "                  #  'F4': pdfeatures3,\n",
    "                #    'F5': pdfeatures4,\n",
    "                #    'F6': pdfeatures5,\n",
    "                #    'F7': pdfeatures6,\n",
    "                #    'F8': pdfeatures7,\n",
    "                #    'F9': pdfeatures8,\n",
    "                #    'F10': pdfeatures9,\n",
    "                #    'F11': pdfeatures10,\n",
    "                #    'F12': pdfeatures11,\n",
    "                #    'F13': pdfeatures12,\n",
    "                #    'F14': pdfeatures13,\n",
    "                   'Labels': labels})\n",
    "df = np.array(features0)#, features1, features2, features3], dtype=float)#, features4, features5, features6, features7, features8, features9, features10, features11, features12, features13], dtype=float)\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    135\n",
       "0     26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(labels[161:]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = torch.tensor(df).float().unsqueeze(1)\n",
    "df_labels = torch.tensor(labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features, df_train_labels, df_test_features, df_test_labels = df_features[:161,:], df_labels[:161], df_features[161:,:], df_labels[161:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "        # batch_first=True (batch_dim, seq_dim, feature_dim)\n",
    "        self.drp = nn.Dropout(0.7)\n",
    "        self.fc = nn.Linear(hidden_d, hidden_d)\n",
    "        self.relu = nn.ReLU(hidden_d)\n",
    "        self.fc2 = nn.Linear(hidden_d, output_d)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.drp(out)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return self.sig(self.fc2(self.relu(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6984350681304932\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6967582702636719\n",
      "Validation Accuracy: 0.16\n",
      "Training loss: 0.6940047740936279\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6898000240325928\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6848922967910767\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6787427067756653\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6721627712249756\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6653454899787903\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6577997207641602\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6501646041870117\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.642573893070221\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6346994042396545\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6268050670623779\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6194712519645691\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6121681332588196\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.6051611304283142\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5979801416397095\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5918235778808594\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5854513645172119\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5796005725860596\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5744274258613586\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5694597363471985\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5648720860481262\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5604572296142578\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5566070079803467\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5529054999351501\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5496008992195129\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.546725332736969\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5439187288284302\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5416300892829895\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5392730236053467\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5371161103248596\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5353524088859558\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5337451696395874\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5320931673049927\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5308498740196228\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5295411348342896\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5287008881568909\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5274384617805481\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5263139605522156\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5257415771484375\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5246749520301819\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5241104960441589\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5235896706581116\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5229529142379761\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5224882960319519\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5220102071762085\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5215196013450623\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5212410688400269\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5206841230392456\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5204238295555115\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5198773741722107\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5195964574813843\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.519605815410614\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5191400051116943\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5189463496208191\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.518536388874054\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5184751749038696\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5183360576629639\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5179966688156128\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5179118514060974\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5178088545799255\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5175355076789856\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5173264741897583\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.517176628112793\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5171364545822144\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5168939232826233\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.516936182975769\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5166997909545898\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.516649067401886\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5164238810539246\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5164780020713806\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.516388475894928\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5161897540092468\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5162863731384277\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5160209536552429\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5160133242607117\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5160492062568665\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5159075856208801\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5158296823501587\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5157084465026855\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5156881213188171\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.515657901763916\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5155478715896606\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5155969262123108\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5153851509094238\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5153692960739136\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5153259038925171\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5154659152030945\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5152978897094727\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5153189897537231\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5151550769805908\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5150741934776306\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5150219202041626\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5150189399719238\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5149097442626953\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5149490237236023\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5148582458496094\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5149049758911133\n",
      "Validation Accuracy: 0.84\n",
      "Training loss: 0.5148534774780273\n",
      "Validation Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_d=512, hidden_d=100, layer_d=100, output_d=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(df_train_features)\n",
    "    loss = criterion(outputs.float(), df_train_labels.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Training loss: {loss}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        outputs = model(df_test_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(df_test_labels, predicted)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(df_test_features)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813],\n",
       "        [0.0187, 0.9813]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
