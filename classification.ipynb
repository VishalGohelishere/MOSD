{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "current_key = None\n",
    "current_values = []\n",
    "\n",
    "with open('Earthquakes_TRAIN.ts', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith('@'):\n",
    "            if current_key is not None:\n",
    "                if len(current_values) == 1:\n",
    "                    data_dict[current_key] = current_values[0]\n",
    "                else:\n",
    "                    data_dict[current_key] = current_values\n",
    "                current_values = []\n",
    "            current_key = line\n",
    "        else:\n",
    "            current_values.append(line)\n",
    "\n",
    "if current_key is not None:\n",
    "    if len(current_values) == 1:\n",
    "        data_dict[current_key] = current_values[0]\n",
    "    else:\n",
    "        data_dict[current_key] = current_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [item.split(':') for item in data_dict['@data']]\n",
    "\n",
    "pdfeatures0 = [list(map(float, item[0].split(','))) for item in split_data]\n",
    "# pdfeatures1 = [list(map(float, item[1].split(','))) for item in split_data]\n",
    "# pdfeatures2 = [list(map(float, item[2].split(','))) for item in split_data]\n",
    "# pdfeatures3 = [list(map(float, item[3].split(','))) for item in split_data]\n",
    "# pdfeatures4 = [list(map(float, item[4].split(','))) for item in split_data]\n",
    "# pdfeatures5 = [list(map(float, item[5].split(','))) for item in split_data]\n",
    "# pdfeatures6 = [list(map(float, item[6].split(','))) for item in split_data]\n",
    "# pdfeatures7 = [list(map(float, item[7].split(','))) for item in split_data]\n",
    "# pdfeatures8 = [list(map(float, item[8].split(','))) for item in split_data]\n",
    "# pdfeatures9 = [list(map(float, item[9].split(','))) for item in split_data]\n",
    "# pdfeatures10 = [list(map(float, item[10].split(','))) for item in split_data]\n",
    "# pdfeatures11 = [list(map(float, item[11].split(','))) for item in split_data]\n",
    "# pdfeatures12 = [list(map(float, item[12].split(','))) for item in split_data]\n",
    "# pdfeatures13 = [list(map(float, item[13].split(','))) for item in split_data]\n",
    "\n",
    "features0 = np.array([list(map(float, item[0].split(','))) for item in split_data])\n",
    "# features1 = np.array([list(map(float, item[1].split(','))) for item in split_data])\n",
    "# features2 = np.array([list(map(float, item[2].split(','))) for item in split_data])\n",
    "# features3 = np.array([list(map(float, item[3].split(','))) for item in split_data])\n",
    "# features4 = np.array([list(map(float, item[4].split(','))) for item in split_data])\n",
    "# features5 = np.array([list(map(float, item[5].split(','))) for item in split_data])\n",
    "# features6 = np.array([list(map(float, item[6].split(','))) for item in split_data])\n",
    "# features7 = np.array([list(map(float, item[7].split(','))) for item in split_data])\n",
    "# features8 = np.array([list(map(float, item[8].split(','))) for item in split_data])\n",
    "# features9 = np.array([list(map(float, item[9].split(','))) for item in split_data])\n",
    "# features10 = np.array([list(map(float, item[10].split(','))) for item in split_data])\n",
    "# features11 = np.array([list(map(float, item[11].split(','))) for item in split_data])\n",
    "# features12 = np.array([list(map(float, item[12].split(','))) for item in split_data])\n",
    "# features13 = np.array([list(map(float, item[13].split(','))) for item in split_data])\n",
    "\n",
    "labels = [0 if int(item[-1]) else 1 for item in split_data]\n",
    "\n",
    "pddf = pd.DataFrame({'F1': pdfeatures0,\n",
    "                  #  'F2': pdfeatures1,\n",
    "                  #  'F3': pdfeatures2,\n",
    "                  #  'F4': pdfeatures3,\n",
    "                #    'F5': pdfeatures4,\n",
    "                #    'F6': pdfeatures5,\n",
    "                #    'F7': pdfeatures6,\n",
    "                #    'F8': pdfeatures7,\n",
    "                #    'F9': pdfeatures8,\n",
    "                #    'F10': pdfeatures9,\n",
    "                #    'F11': pdfeatures10,\n",
    "                #    'F12': pdfeatures11,\n",
    "                #    'F13': pdfeatures12,\n",
    "                #    'F14': pdfeatures13,\n",
    "                   'Labels': labels})\n",
    "df = np.array(features0)#, features1, features2, features3], dtype=float)#, features4, features5, features6, features7, features8, features9, features10, features11, features12, features13], dtype=float)\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    135\n",
       "0     26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(labels[161:]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = torch.tensor(df).float().unsqueeze(1)\n",
    "df_labels = torch.tensor(labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features, df_train_labels, df_test_features, df_test_labels = df_features[:161,:], df_labels[:161], df_features[161:,:], df_labels[161:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "        # batch_first=True (batch_dim, seq_dim, feature_dim)\n",
    "        self.drp = nn.Dropout(0.7)\n",
    "        self.fc = nn.Linear(hidden_d, output_d)\n",
    "        self.relu = nn.ReLU(hidden_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.drp(out)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'return_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vishal Gohel\\Vishal Gohel\\Models of Sequential Data\\project\\classification.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMModel(input_d\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, hidden_d\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, layer_d\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, output_d\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Vishal Gohel\\Vishal Gohel\\Models of Sequential Data\\project\\classification.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_dim \u001b[39m=\u001b[39m layer_d\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# LSTM model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLSTM(input_d, hidden_d, layer_d, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# batch_first=True (batch_dim, seq_dim, feature_dim)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vishal%20Gohel/Vishal%20Gohel/Models%20of%20Sequential%20Data/project/classification.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrp \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.7\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Vishal Gohel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:711\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 711\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'return_sequences'"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_d=512, hidden_d=100, layer_d=100, output_d=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(df_train_features)\n",
    "    loss = criterion(outputs.float(), df_train_labels.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Training loss: {loss}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        outputs = model( df_test_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(df_test_labels, predicted)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
